{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A continuación se describen los principales metodos de depuración y control de archivos a emplear en cualquier notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas Cheats Sheet\n",
    "link: https://www.dataquest.io/blog/pandas-cheat-sheet/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Procesamientos con expresiones regulares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re #libreria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tratamiento de archivos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'archivo.xlsx'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patttern_to_delete = r'(\\d{1,}\\w|_)|(centrodese[A-Z][0-9]|centrodese)' # Ejemplo, si quiero quitar digitos seguidos de \"_\", y palabras especificas\n",
    "texts = '1_144a_archivo_centrodeseT6.xlsx'\n",
    "re.sub(patttern_to_delete,'',texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Definición de Patrones Especificos que se desea buscar**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern_for_it = re.compile(r'PA[0-9]{4,5}|IT[0-9]{4,5}|pa[0-9]{4,5}|it[0-9]{4,5}|P[0-9]{3,4}|p[0-9]{3,4}',re.X) #uso el compilador Verbose, para usar el /b \"bounday de palabras\"\n",
    "pattern_for_field = re.compile(r'\\b(?!CENTRAL|PERSON|FALSO|\\n)[A-Z]{4,}\\b',re.X)\n",
    "pattern_for_country = re.compile(r'[A-Z]{4,}-MX|[A-Z]{4,}-AR|[A-Z]{4,}-NI|[A-Z]{4,}-CO|[A-Z]{4,}-COL|[A-Z]{4,}-NI|[A-Z]{4,}-BR|[A-Z]{4,}-VZ|[A-Z]{4,}-PA|[A-Z]{4,}-UY',re.X)\n",
    "#el primer pattern busca, los PAy que acontinuación tengan cierta cantidad de caracteres, luego, separado por | los IT, con la misma estructura, \n",
    "#el segundo pattern, busco las palabras que empiecen en mayuscuca, a partir de 4 caracteres en adelante y excluyo ciertas palabras\n",
    "#en el pattern de pais, solo busco los paises de forma literal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Funciones helpers**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Funcion para encontrar los patternes definidos en una columna de un dataframe e incorporación al row, concatenado*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_patterns_in_columns(row, infotype, pattern):\n",
    "    a = re.findall(pattern,str(row))\n",
    "    s = ',' #string para hacer el join\n",
    "    data = s.join(a)\n",
    "    if infotype != \"\" or infotype != None:\n",
    "        all_data = s.join([data,infotype])\n",
    "        return all_data\n",
    "    else:\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*convertir un valor encontrado en un texto, segun diccionario, y devolver otro y completarlo en la columna requerida*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defino un diccionario cualquiera, con clave y valor\n",
    "countries_dict = {'Brasil': 'BR','Venezuela': 'VZ', 'México': 'MX', 'Colombia': 'CO', 'Filipinas': 'FI', 'Costa Rica':'CR', 'Guatemala': 'GM', 'Nicaragua': 'NI', 'Panamá' : 'PA','Argentina': 'AR'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_text_to_iso_country(row, column):\n",
    "    s = ','\n",
    "    results = []\n",
    "    for k,v in countries_dict.items():\n",
    "        pattern = re.compile(f'{k}')\n",
    "        check = pattern.findall(row)\n",
    "        if check:\n",
    "            results.append(v)\n",
    "    output = s.join(results)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementación de la funciónn de patterns para todas las columnas del DF. Esto dependerá de la estructura del dataframe. En este ejemplo, se definio aplicar este metodo a **todas las columnas**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df.columns:\n",
    "    df['infotipo'] = df.apply(lambda row: search_patterns_in_columns(row[col],row['infotipo'],pattern_for_it),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reemplazo de un ID Viejo por uno Nuevo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El valor del row es convertido a string, y luego compilado y buscado en el valor del row. Si se encuentra, se agrega a una lista de resultados. Finalmente, para evitar duplicaciones, o la incorporacion de valores incorrectos. Se evalua si el Result (lista) tiene un valor mayor a 0, con lo cual, esto implica que existe un valor nuevo a devolver, caso contrario devuelvo el valor viejo denominado \"old_id\"\n",
    "\n",
    "*old_id_new_id_dic* Representa una lista con el valor viejo y nuevo mapeado:\n",
    "\n",
    "old_id_new_id_dic = {\n",
    "\n",
    "'6001701':'6009800',\n",
    "'6001702':'6009801',\n",
    "'6001703':'6009802',\n",
    "'6001704':'6009803',\n",
    "'6001705':'6009804',\n",
    "'6001706':'6009805',\n",
    "'6001707':'6009806',\n",
    "'6001708':'6009807',\n",
    "'6001709':'6009808',\n",
    "'6001710':'6009809',\n",
    "'6001711':'6009810',\n",
    "'6001712':'6009811',\n",
    "'6001713':'6009812',\n",
    "'6001714':'6009813',\n",
    "'6001715':'6009814'\n",
    "\n",
    "}\n",
    "\n",
    "*Se implementa con una función anonima para el valor deseado* en este caso se desea Actualizar el valor en userId\n",
    "\n",
    "**df['TEST_USERID'] =  df.apply(lambda row: update_ids(row['userId']),axis=1)**\n",
    "\n",
    "**NOTA: el buscar una expresion regular, si el campo tiene otros caracteres, solo sera reemplazado el ID especificado, dejando el resto del string sin actualizar, esto es util al emplearlo para el valor del userid de Time Account**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def old_id_new_id(row):\n",
    "    old_id = str(row)\n",
    "    result = []\n",
    "    s = ','\n",
    "    for k,v in old_id_new_id_dic.items():\n",
    "        pattern = re.compile(f'{k}',re.X)\n",
    "        check_pattern = pattern.search(old_id)\n",
    "        if check_pattern:\n",
    "            new_id = pattern.sub(v,old_id)\n",
    "            result.append(new_id)\n",
    "    if len(result) != 0:\n",
    "        output = s.join(result)\n",
    "        return output\n",
    "    else:\n",
    "        return old_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Unificación de archivos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unificación de archivos excels en uno solo, agregando en una columna el nombre de los archivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_xls = []\n",
    "\n",
    "for f in files:\n",
    "    if f.endswith(\".xlsx\"):\n",
    "        fff = employee_data_path_files + f\n",
    "        files_xls.append(fff)\n",
    "\n",
    "df = pd.DataFrame();\n",
    "\n",
    "for f in files_xls:\n",
    "    data = pd.read_excel(f, skip_footer=0)\n",
    "    file_date = os.path.basename(f).replace(\".xlsx\", \"\")\n",
    "    data.index = [file_date] * len(data) #genero el index con el nombre del archivo\n",
    "    df = df.append(data)\n",
    "\n",
    "df.to_excel(path_transported + 'unify_employee_data_map_JJ.xlsx', merge_cells = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unificación de todas las hojas de un archivo excel en una sola"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = pd.concat(pd.read_excel(path2+'EmployeeDataTemplates - 20200703.xlsx', sheet_name=None), ignore_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.to_excel(path2+'all_data_employeeDataTemplate_test_delete.xlsx') # guardo archivo unificado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3)  Control de consistencia entre dos dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considerando el DF1 como Izquierda y el DF2 como deferecha, se genera un nuevo DF, con los valores que no existen en ambos (distinto de \"both\"), devolviendo las diferencias ya sea de Izquierda o Derecha segun se reguiera, definiendo este parametro en la propiedad \"HOW\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.merge(df2,indicator = True, how='right').loc[lambda x : x['_merge']!='both']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Modificaciones de dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejemplo de cuando se quiere generar Rows nuevos a partir de distintos valores separados por \",\" en una Celda. Ej:\n",
    "\n",
    "[\"valor1\",\"Valor2] --> Original\n",
    "\n",
    "\n",
    "Resultado Final\n",
    "\n",
    "\n",
    "[\"valor1]\n",
    "\n",
    "\n",
    "[\"valor2\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-02e40b2eb888>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_transported\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'pre-proceded-all-employee-data_test.xlsx'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "df = pd.read_excel(path_transported + 'file.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['campo'] = df['campo'].str.split(',') #hago un split de la celda que tiene el dato separado por \",\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = df.apply(pd.Series.explode) #hago un explode de la serie para generar nuevos rows con la columna seleccionada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.to_excel(path_transported + 'file.xlsx') #finalmente guardo el archivo con los nuevos rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transpose de DT y reseteo de Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = df.T.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Incorporación de columnas vacias con nombres especificos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inserto en una posición especifica una columna\n",
    "df.insert(2,'nuevacolumna1','')\n",
    "df.insert(3,'nuevacolumna2','')\n",
    "df.insert(4,'nuevacolumna3','')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generación de dos columnas, a partir del split de una columna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['nuevacolumna1','nuevacolumna2']] = df.Campo.str.split('-',expand=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reemplazo del valor de una columna, a partir de un string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['nuevacolumna2'] = df.infotipo.str.replace('PA','')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definición de nuevos nombres de columnas en un DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.set_axis([\"columna1\", \"columna2\", \"columna3\",\"columna4\",\"columna5\",\"columna6\"], axis=1, inplace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Agregar valores a un DF a una columna especifica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in enumerate(cecos):\n",
    "        df.at[k+1,'columna_requerida'] = v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### completar con un valor 0 o 1 a partir del contenido de un string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lista de paises, es un array con las listas de paises\n",
    "lista_paises = ['pais 1', 'pais 2', 'pais 3']\n",
    "\n",
    "def add_value_to_column_country_not_apply_to(df):\n",
    "    for p in lista_paises:\n",
    "        df[p] = np.where(df['columna1'].str.contains(p, na=False), 0,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quitar caracteres extraños en todas las columnas\n",
    "\n",
    "aplicar este proceso a todas las columnas del DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in columns:\n",
    "    df[c] = df[c].str.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Filtros de Datos**\n",
    "\n",
    "Se define un filtro de datos, y se aplica la busqueda con el metodo \"isin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workers = ['1','2','3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df[df.worker.isin(workers)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5)  Quitar duplicados en datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Función que se utilizo para quitar duplicados de una celda, que tenia los datos en String separados por \",\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deduplicate_values(x):\n",
    "    data = x.split(',') \n",
    "    s = ','\n",
    "    d_duplicate = list(dict.fromkeys(data)) #elimino los duplicados\n",
    "    data_depured = s.join(d_duplicate) #paso a string la lista\n",
    "    return data_depured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ejemplo de implementación de función\n",
    "df['columna1'] = df.apply(lambda row: deduplicate_values(row['columna1']),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Control de diferencias entre listas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Diferencia entre listados de archivos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "files_differences =  set(lista_1_archivos) - set(lista_2_archivos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_differences =  set(dataset_one) - set(dataset_two)\n",
    "list_difference_control_two = list(files_differences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Archivos en comunes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "set(lista_1_archivos).intersection(lista_2_archivos))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si la función devuelve un **array vacio**, es que no hay archivos en comun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(list_of_files_one).intersection(list_of_files_two))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7) Lectura de archivo CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#forma 1\n",
    "df = pd.read_csv(path+'file.csv',sep=r',', engine='python') #caso contratio tira un error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#forma 2\n",
    "df = pd.read_csv(root_resultados+'file.csv',delimiter=',',skiprows=1) #salteo la primer fila"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8) Guardar un archivo CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(path+'new_file.csv',index=False, encoding='utf-8') # considerar el encoding, encado se emplear el archivo en otros sistemas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9) Tratamiento de strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = 'FEMX0550COEL27'\n",
    "num = str(texts[4:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0550'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0550'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num.replace(f'^[/]','F')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**agregar '0' a la izquierda, de un string**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'00000001'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(num).zfill(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10) Procesamiento de Archivos Excels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Obtener los sheetsname de archivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-86fdacfbac44>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mxl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExcelFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'foo.xls'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mxl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msheet_names\u001b[0m  \u001b[1;31m# see all sheet names\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mxl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msheet_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "xl = pd.ExcelFile('foo.xls')\n",
    "xl.sheet_names  # see all sheet names\n",
    "xl.parse(sheet_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Procesas todos los sheets de un archivo Excel\n",
    "\n",
    "Cada sheet es una key de un diccionario\n",
    "\n",
    "dict_keys(['HRP1000', 'HRP1001', 'HRP1002', 'HRP1005', 'HRP1007', 'HRP1008', 'HRP1013', 'HRP9045', 'HRP9047', 'Hoja1'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(path+file,sheet_name=None)\n",
    "df.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dfs['Sheet1'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11) Extraccion de Información de un Archivo, a partir de la definición de las columnas a extraer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">!!! Importante: </span> Estos pasos aplican para un archivo con multiples hojas, sobre lo cual se quiere unificar\n",
    "\n",
    "En la lista <span style=\"color:green\">**columns_to_colect**</span> defino las columnas que quiero extraer\n",
    "\n",
    "La función <span style=\"color:green\">**\"getColumnNameWithValue**</span> identifica en las columnas del DF si esos valores existen en alguna de las columnas. Esto se da principalemnte en archivos que tienen doble cabecera, o la columna inicia en un row con index distinto de 0.Caso contrario no es necesaria esta operación. Devuelve un array zipeado, para ser utilizado en la siguiente funcion <span style=\"color:green\">**transform_data_colected_into_df**</span>.\n",
    "\n",
    "La función <span style=\"color:green\">**transform_data_colected_into_df**</span> devuelve el nuevo DataFrame con las columnas identificadas en <span style=\"color:green\">**columns_to_colect**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_colect =  ['column1','column2']\n",
    "\n",
    "#Funcion Auxiliar \n",
    "def getColumnNameWithValue ( df , columnValues, sheetname=''):\n",
    "    columns_to_filters_from_df = []\n",
    "    rows_index_to_filter = []\n",
    "    sheetnameWithValue = [] # se agrega para procesar el sheetname que corresponde, si no tiene valor\n",
    "    for column in df.columns:\n",
    "        values_to_check = df[column].values\n",
    "        for val in values_to_check:\n",
    "            for colValue in columnValues:\n",
    "                if val == colValue:\n",
    "                    columns_to_filters_from_df.append(column)\n",
    "                    ind = df.index[df[column]==val].tolist()\n",
    "                    rows_index_to_filter.append(ind[0])\n",
    "                    sheetnameWithValue.append(sheetname)\n",
    "    index_row_and_column = zip( sheetnameWithValue, rows_index_to_filter, columns_to_filters_from_df)\n",
    "    return index_row_and_column\n",
    "\n",
    "#funcion auxiliar\n",
    "def transform_data_colected_into_df( df, sheet_name, file_name, columnData, column_to_colect):\n",
    "    \n",
    "    d = dict.fromkeys(column_to_colect, 0)\n",
    "    d['sheetname'] = 0\n",
    "    d['filename'] = 0\n",
    "    \n",
    "    data_colected = []\n",
    "    \n",
    "    for sheetname, row,col in columnData:\n",
    "        if sheetname == sheet_name:\n",
    "            dat = list(df.loc[row+1:,col])\n",
    "            data_colected.append(dat)\n",
    "        else:\n",
    "            pass\n",
    "    for k,v in zip(d,data_colected):\n",
    "        d[k] = v\n",
    "        d['sheetname'] = sheet_name\n",
    "        d['filename'] = file_name\n",
    "    #finalmente creo el nuevo dataFrame que ire almacenando en un repositorio para finalmente concatenar todo en un único archivo\n",
    "    if len(data_colected) != 0:\n",
    "        new_df = pd.DataFrame(d)\n",
    "        return new_df\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "dataFrames_to_concat = []\n",
    "\n",
    "#procesamiento de los archivos\n",
    "for sheetname in sheetnames :\n",
    "    print('procesando sheetname {} de archivo {}'.format(sheetname, core_files[2]))\n",
    "    df = pd.read_excel( root_path + core_files[2] , sheet_name=sheetname)\n",
    "    colval = getColumnNameWithValue(df,columns_to_colect,sheetname)\n",
    "    proceded_new_df = transform_data_colected_into_df(df,sheetname,core_files[2],colval,columns_to_colect)\n",
    "    dataFrames_to_concat.append(proceded_new_df)\n",
    "\n",
    "final_df = pd.concat(dataFrames_to_concat, ignore_index=True, axis=0)\n",
    "\n",
    "final_df.to_excel( output +'final_file_with_column_required.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trabajando con archivos ZIPEADOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipfile import ZipFile "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtener directorio de un archivo zipeado\n",
    "with ZipFile(root+files[3], 'r') as zip:\n",
    "    zip.printdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## leear un archivo csv en un archivo zipeado\n",
    "df = ''\n",
    "with ZipFile(root+files[3], 'r') as zip:\n",
    "    df = pd.read_csv(zip.open('Benefit Enrollment.csv'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
